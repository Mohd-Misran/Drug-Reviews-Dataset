{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Drug Reviews - Notebook 2 (Data Pre-Processing).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTj9nZSKWGrs",
        "colab_type": "text"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DBS3Sh-uhD6",
        "colab_type": "code",
        "outputId": "335dbb20-2702-47be-9944-ea3c56e406d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAH8liZwWKDT",
        "colab_type": "text"
      },
      "source": [
        "**Load Dataset**\n",
        "> Here, I am loading the concatenated dataset from the previous notebook so it's easy to process the data all-together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW1_Hmty3flI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Drug Reviews Dataset/DrugReviews.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOECAn7zYCGj",
        "colab_type": "text"
      },
      "source": [
        "**Clean Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_13z6m1_XYNC",
        "colab_type": "text"
      },
      "source": [
        "From the previous notebook, it was known that there were 1194 NULL values in the *condition* column. Let's drop it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3bv_Mhw4Soz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvnH_LPlXl1R",
        "colab_type": "text"
      },
      "source": [
        "Now, I'm gonna get rid of the rows which had *condition* values containing < /span>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKqxfV6w4iw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "span_list = list()\n",
        "for i, condition in enumerate(df['condition']):\n",
        "  if '</span>' in condition:\n",
        "    span_list.append(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPk2nl6044aM",
        "colab_type": "code",
        "outputId": "811265d3-43ab-45ff-824a-ac62d8cef7b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(span_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-z5pKn8X0ph",
        "colab_type": "text"
      },
      "source": [
        "There 1171 rows in which the *condition* value contains < /span>. I have fetched the indices of those rows and now it'll be easy to drop them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uh6WMSw45Ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(index=span_list)\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CIakSUq5FhY",
        "colab_type": "code",
        "outputId": "19ba942b-284a-4582-9af8-322a416dac2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(any(['</span>' in c for c in df['condition']]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht4WC9DiYRw8",
        "colab_type": "text"
      },
      "source": [
        "Now you can see that the *condition* column does not have any values containing < /span>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxcJhm_AYnZd",
        "colab_type": "text"
      },
      "source": [
        "**Review Pre-Processing**\n",
        "> For efficient Sentiment Analysis, you need text data which can be easily transformed to computer understandable vectors. For doing that, you mostly need to get rid of all the unnecessary characters and just include the required words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfs9lhxmY04w",
        "colab_type": "text"
      },
      "source": [
        "Stop words are a set of words most commonly used in any language. So you might wanna get rid of them, otherwise your vocabulary will be very bulky."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WirjcoCfJplq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stops = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kryD-kkpKpoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_stops = [\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",\"no\",\"nor\",\"not\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwX5BiJAMK3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for not_stop in not_stops:\n",
        "  stops.remove(not_stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_XW9horZNoD",
        "colab_type": "text"
      },
      "source": [
        "I have excluded some of the stop words (stored in *not_stops*) as these words can be essential in sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfURsJMmZ_gk",
        "colab_type": "text"
      },
      "source": [
        "Stemming:\n",
        "> Stemming is a process of reducing derived words to their root form. Again, this is helpful in keeping a less bulky vocabulary. I'll tell you how. Consider an example of the words *clean*, *cleaning* and *cleaned*. These three words have the same meaning and just differ in the tense. Now without stemming, these three words would be unique in the vocabulary and would have different representations. But, thanks to Stemming, you can represent these words with just one representation (say, *clean*). So, in this way, you can represent a bunch of similar words with their root."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i51VCZJEbl4n",
        "colab_type": "text"
      },
      "source": [
        "You can use the inbuilt Snowball Stemmer from the NLTK library to do your job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ57qBMvRH87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0x7MdE4RRvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_reviews(raw_text):\n",
        "\n",
        "  # As the data has been scraped, it will have some HTML. We will get rid of that using BeautifulSoup.\n",
        "  text = soup(raw_text, 'html.parser').get_text()\n",
        "\n",
        "  # Remove punctuation marks and retain just the words\n",
        "  punct_removed = re.sub('[^a-zA-Z0-9\\']', ' ', text)\n",
        "\n",
        "  # Convert all words to lower case\n",
        "  words = punct_removed.lower().split()\n",
        "\n",
        "  # Fetch only meaningful words\n",
        "  required_words = [word for word in words if word not in stops]\n",
        "\n",
        "  # Stem similar words\n",
        "  stemmed = [stemmer.stem(rw) for rw in required_words]\n",
        "  \n",
        "  # Return the cleaned text\n",
        "  return \" \".join(stemmed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVfkUotEcod-",
        "colab_type": "text"
      },
      "source": [
        "Apply the *clean_reviews* function to the *review* column and store the cleaned reviews in a new column *cleaned_reviews*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JglWSv9lTPxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['cleaned_reviews'] = df['review'].apply(clean_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZZZGw9wc0Jn",
        "colab_type": "text"
      },
      "source": [
        "Uploading the two columns *cleaned_reviews* and *sentiment* as a DataFrame onto my drive for building a Deep Learning Model with Word Embeddings in the next notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAG1HEOBU3pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['cleaned_reviews','sentiment']].to_csv('DrugReviewsSentimentAnalysis.csv')\n",
        "!cp DrugReviewsSentimentAnalysis.csv \"drive/My Drive/Drug Reviews Dataset/\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}